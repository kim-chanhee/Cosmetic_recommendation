# -*- coding: utf-8 -*-
"""
OliveYoung 'ì—¬ë“œë¦„' í¬ë¦¼ ë‹¤ê±´ + ë¦¬ë·° ì „ëŸ‰(ëê¹Œì§€) í¬ë¡¤ë§
- ìƒí’ˆ ëª©ë¡: startCount â†’ ë¶€ì¡±ì‹œ í˜ì´ì§€ë„¤ì´í„° fallback
- ë¦¬ë·°: ì„±ë³„(ì—¬ì„±â†’ë‚¨ì„±) í•„í„° ê°ê° ëê¹Œì§€ ìˆ˜ì§‘
- ìŠ¤í‚¨ íƒœê·¸ 3ë¶„í• : skin_type / skin_tone / skin_concerns
- í‰ì (ì›ë¬¸/ìˆ«ì) í¬í•¨, ë“œë¼ì´ë²„ ì˜ˆì™¸ ì‹œ ì¬ìƒì„± í›„ 1íšŒ ì¬ì‹œë„
- CSV ì»¬ëŸ¼:
  product_name, product_brand, product_link, customer_name,
  skin_type, skin_tone, skin_concerns, review, date,
  rating_text, rating, gender
"""

import os, re, csv, time, random
from urllib.parse import quote

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    TimeoutException, NoSuchElementException, WebDriverException,
    StaleElementReferenceException, InvalidSessionIdException
)

# =========================
# 0) ë“œë¼ì´ë²„/í™˜ê²½ ì„¤ì •
# =========================
def make_driver():
    options = webdriver.ChromeOptions()
    # options.add_argument("--headless")  # í•„ìš” ì‹œ í™œì„±í™”
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--lang=ko-KR")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119 Safari/537.36"
    )
    d = webdriver.Chrome(options=options)
    w = WebDriverWait(d, 12)
    return d, w

driver, wait = make_driver()

# =========================
# 1) íŒŒë¼ë¯¸í„°/ê²½ë¡œ
# =========================
keyword = "ì—¬ë“œë¦„"
items_per_page = 48
MAX_STARTCOUNT_PAGES = 1  # startCount ë°©ì‹ ìµœëŒ€ í˜ì´ì§€ìˆ˜
PAGINATOR_MAX_CLICKS  = 60

START_AT = 0
MAX_PRODUCTS = None

BASE_DIR = "/Users/Shared/ìµœì¢…ì„ _êµìˆ˜ë‹˜/Face_skin_disease/trash/crawlcode/í¬ë¡¤ë§data"
CSV_PATH = os.path.join(BASE_DIR, "ì—¬ë“œë¦„_í¬ë¦¼_reviews_flat.csv")
PRODUCT_LIST_CSV = os.path.join(BASE_DIR, "ì—¬ë“œë¦„_í¬ë¦¼_list.csv")
os.makedirs(BASE_DIR, exist_ok=True)

# =========================
# 2) ìœ í‹¸
# =========================
def sleep_smart(a=1.0, b=2.0):
    time.sleep(random.uniform(a, b))

def parse_rating_to_float(text: str):
    if not text:
        return None
    nums = re.findall(r'(\d+(?:\.\d+)?)', text)
    if not nums:
        return None
    try:
        return float(nums[-1])
    except:
        return None

def safe_click(el):
    try:
        driver.execute_script("arguments[0].click();", el)
        return True
    except Exception:
        try:
            el.click()
            return True
        except Exception:
            return False

def recreate_driver():
    global driver, wait
    try:
        driver.quit()
    except Exception:
        pass
    driver, wait = make_driver()

# =========================
# 3) ìŠ¤í‚¨ íƒœê·¸ ë¶„ë¦¬(í”¼ë¶€íƒ€ì…/í”¼ë¶€í†¤/í”¼ë¶€ê³ ë¯¼)
# =========================
SKIN_TYPE_SET = {"ì§€ì„±","ê±´ì„±","ë³µí•©ì„±","ë¯¼ê°ì„±","ì•½ê±´ì„±","íŠ¸ëŸ¬ë¸”ì„±","ì¤‘ì„±"}
SKIN_TONE_SET = {"ì¿¨í†¤","ì›œí†¤","ë´„ì›œí†¤","ì—¬ë¦„ì¿¨í†¤","ê°€ì„ì›œí†¤","ê²¨ìš¸ì¿¨í†¤","ë´„ì›í†¤"}
SKIN_CONCERN_SET = {
    "ì¡í‹°","ë¯¸ë°±","ì£¼ë¦„","ê°ì§ˆ","íŠ¸ëŸ¬ë¸”","ë¸”ë™í—¤ë“œ","í”¼ì§€ê³¼ë‹¤","ë¯¼ê°ì„±",
    "ëª¨ê³µ","íƒ„ë ¥","í™ì¡°","ì•„í† í”¼","ë‹¤í¬ì„œí´"
}
ALIAS = {
    "ë´„ì›í†¤":"ë´„ì›œí†¤",
    "ì—¬ë“œë¦„":"íŠ¸ëŸ¬ë¸”",
    "ì—¬ë“œë¦„ì„±":"íŠ¸ëŸ¬ë¸”ì„±",
    "íŠ¸ëŸ¬ë¸”ì„±í”¼ë¶€":"íŠ¸ëŸ¬ë¸”ì„±",
    "ë¯¼ê°ì„±í”¼ë¶€":"ë¯¼ê°ì„±",
}

def _norm_token(s: str) -> str:
    t = (s or "").strip().replace(" ", "")
    return ALIAS.get(t, t)

def split_skin_tags(span_texts):
    skin_type = ""
    skin_tone = ""
    concerns = []
    seen = set()
    for raw in span_texts:
        tok = _norm_token(raw)
        if not tok or tok in seen:
            continue
        seen.add(tok)
        if not skin_type and tok in SKIN_TYPE_SET:
            skin_type = tok
            continue
        if not skin_tone and tok in SKIN_TONE_SET:
            skin_tone = "ë´„ì›œí†¤" if tok == "ë´„ì›í†¤" else tok
            continue
        if tok in SKIN_CONCERN_SET:
            concerns.append(tok)
    if skin_type == "ë¯¼ê°ì„±":
        concerns = [c for c in concerns if c != "ë¯¼ê°ì„±"]
    return skin_type, skin_tone, (" / ".join(concerns) if concerns else "")

# =========================
# 4) CSV ì´ˆê¸°í™” / Append  (gender ë° 3ì»¬ëŸ¼ í¬í•¨)
# =========================
def init_reviews_csv(path=CSV_PATH):
    with open(path, "w", newline="", encoding="utf-8-sig") as fw:
        writer = csv.DictWriter(
            fw,
            fieldnames=[
                "product_name","product_brand","product_link",
                "customer_name",
                "skin_type","skin_tone","skin_concerns",
                "review","date","rating_text","rating","gender"
            ]
        )
        writer.writeheader()

def append_reviews_to_csv(product, reviews, path=CSV_PATH):
    if not reviews:
        return
    with open(path, "a", newline="", encoding="utf-8-sig") as fa:
        writer = csv.DictWriter(
            fa,
            fieldnames=[
                "product_name","product_brand","product_link",
                "customer_name",
                "skin_type","skin_tone","skin_concerns",
                "review","date","rating_text","rating","gender"
            ]
        )
        for r in reviews:
            writer.writerow({
                "product_name":  product.get("product_name",""),
                "product_brand": product.get("product_brand",""),
                "product_link":  product.get("product_link",""),
                "customer_name": r.get("customer_name",""),
                "skin_type":     r.get("skin_type",""),
                "skin_tone":     r.get("skin_tone",""),
                "skin_concerns": r.get("skin_concerns",""),
                "review":        r.get("review",""),
                "date":          r.get("date",""),
                "rating_text":   r.get("rating_text",""),
                "rating":        r.get("rating",""),
                "gender":        r.get("gender",""),
            })

def write_product_list_csv(products, path=PRODUCT_LIST_CSV):
    with open(path, "w", newline="", encoding="utf-8-sig") as fw:
        writer = csv.DictWriter(fw, fieldnames=["product_name","product_brand","product_link"])
        writer.writeheader()
        for p in products:
            writer.writerow(p)

# =========================
# 5) ìƒí’ˆ ëª©ë¡ (startCount â†’ paginator fallback)
# =========================
def wait_cards_loaded(timeout=12):
    end = time.time() + timeout
    sels = [
        "ul#w_cate_prd_list li.flag.li_result",
        "ul.cate_prd_list li",
        "div.prd_info .tx_name",
    ]
    while time.time() < end:
        for sel in sels:
            if driver.find_elements(By.CSS_SELECTOR, sel):
                return True
        driver.execute_script("window.scrollBy(0, 900);")
        sleep_smart(0.2, 0.4)
    return False

def parse_product_cards():
    cards = driver.find_elements(By.CSS_SELECTOR, "ul#w_cate_prd_list li.flag.li_result")
    if not cards:
        cards = driver.find_elements(By.CSS_SELECTOR, "ul.cate_prd_list li")
    out = []
    for c in cards:
        try:
            try:
                name = c.find_element(By.CSS_SELECTOR, "div.prd_info .tx_name").text.strip()
            except NoSuchElementException:
                name = "N/A"
            try:
                brand = c.find_element(By.CSS_SELECTOR, "div.prd_info .tx_brand").text.strip()
            except NoSuchElementException:
                brand = "N/A"
            try:
                link = c.find_element(By.CSS_SELECTOR, "a").get_attribute("href") or ""
            except NoSuchElementException:
                link = ""
            if link:
                out.append({"product_name": name, "product_brand": brand, "product_link": link})
        except Exception:
            continue
    return out

def build_search_url(start_count=0):
    q = quote(keyword)
    return (
        "https://www.oliveyoung.co.kr/store/search/getSearchMain.do?"
        f"startCount={start_count}"
        "&sort=RANK%2FDESC&goods_sort=WEIGHT%2FDESC%2CRANK%2FDESC"
        "&collection=ALL&reQuery="
        "&viewtype=image&category=&catename=LCTG_ID&catedepth=1&rt="
        f"&listnum={items_per_page}&tmp_requery=&tmp_requery2="
        "&categoryDepthValue=2&cateId=10000010001&cateId2=100000100010015"
        "&BenefitAll_CHECK="
        f"&query={q}&realQuery={q}"
        "&selectCateNm=%ED%81%AC%EB%A6%BC+%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC%EC%97%90"
        "&typeChk=thum"
    )

def crawl_product_list_startcount():
    products, seen = [], set()
    for i in range(MAX_STARTCOUNT_PAGES):
        start_count = i * items_per_page
        try:
            driver.get(build_search_url(start_count))
        except WebDriverException as e:
            print(f"âŒ ì´ë™ ì‹¤íŒ¨(startCount={start_count}): {e}")
            continue
        if not wait_cards_loaded(10):
            print(f"âŒ ë¦¬ìŠ¤íŠ¸ ë¡œë”© íƒ€ì„ì•„ì›ƒ (startCount={start_count})")
            continue
        cards = parse_product_cards()
        added = 0
        for p in cards:
            key = (p["product_name"], p["product_link"])
            if key not in seen:
                seen.add(key); products.append(p); added += 1
        print(f"âœ… startCount={start_count} ìˆ˜ì§‘: {len(cards)}ê°œ (ì‹ ê·œ {added})")
        if added == 0 and i > 0:
            break
    return products

def crawl_product_list_paginator():
    products, seen = [], set()
    try:
        driver.get(build_search_url(0))
    except WebDriverException as e:
        print(f"âŒ ì²« í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
        return products
    if not wait_cards_loaded(10):
        print("âŒ ì²« í˜ì´ì§€ ì¹´ë“œ ë¡œë”© ì‹¤íŒ¨"); return products
    for p in parse_product_cards():
        key = (p["product_name"], p["product_link"])
        if key not in seen:
            seen.add(key); products.append(p)
    clicks = 0
    while clicks < PAGINATOR_MAX_CLICKS:
        pager_links = driver.find_elements(By.CSS_SELECTOR, "div.pageing a")
        if not pager_links: break
        next_clicked = False
        for a in pager_links:
            label = (a.get_attribute("aria-label") or a.text or "").strip()
            if "ë‹¤ìŒ" in label or "next" in label.lower():
                if safe_click(a):
                    clicks += 1; sleep_smart(1.0, 1.6)
                    if wait_cards_loaded(10):
                        added_here = 0
                        for p in parse_product_cards():
                            key = (p["product_name"], p["product_link"])
                            if key not in seen:
                                seen.add(key); products.append(p); added_here += 1
                        next_clicked = True
                break
        if not next_clicked:
            num_links = []
            for a in driver.find_elements(By.CSS_SELECTOR, "div.pageing a"):
                t = (a.text or "").strip()
                if t.isdigit(): num_links.append((int(t), a))
            num_links.sort(key=lambda x: x[0])
            clicked_numeric = False
            for _, a in reversed(num_links):
                if safe_click(a):
                    clicks += 1; sleep_smart(1.0, 1.6)
                    if wait_cards_loaded(10):
                        added_here = 0
                        for p in parse_product_cards():
                            key = (p["product_name"], p["product_link"])
                            if key not in seen:
                                seen.add(key); products.append(p); added_here += 1
                        clicked_numeric = added_here > 0
                    break
            if not clicked_numeric: break
    print(f"âœ… í˜ì´ì§€ë„¤ì´í„° ë°©ì‹ ìˆ˜ì§‘ ì™„ë£Œ: {len(products)}ê°œ")
    return products

def crawl_product_list():
    products = crawl_product_list_startcount()
    if len(products) <= items_per_page:
        print("â„¹ï¸ startCount ê²°ê³¼ê°€ ì ì–´ í˜ì´ì§€ë„¤ì´í„°ë¡œ ì¬ì‹œë„")
        products = crawl_product_list_paginator()
    unique, seen = [], set()
    for p in products:
        key = (p["product_name"], p["product_link"])
        if key not in seen:
            seen.add(key); unique.append(p)
    print(f"âœ… ìµœì¢… ìƒí’ˆ ìˆ˜ì§‘: {len(unique)}ê°œ")
    return unique

# =========================
# 6) ë¦¬ë·° í•„í„°(ì„±ë³„) ì ìš© (nth-child ê¸ˆì§€, ì†ì„± ê¸°ë°˜ + ì ìš©ë²„íŠ¼ ë‹¤ê°ë„ ì‹œë„)
# =========================
def open_filter_panel():
    btn = None
    try:
        btn = wait.until(EC.element_to_be_clickable((By.ID, "filterBtn")))
    except TimeoutException:
        pass
    if not btn:
        cand = driver.find_elements(By.XPATH, "//button[contains(., 'ë¦¬ë·° ê²€ìƒ‰ í•„í„°')]")
        if cand: btn = cand[0]
    if btn:
        driver.execute_script("arguments[0].click();", btn)
        sleep_smart(0.4, 0.8)

def ensure_first_review_page():
    """í•„í„° ì ìš© í›„ 1í˜ì´ì§€ë¡œ ê°•ì œ ì´ë™(ìˆìœ¼ë©´)."""
    try:
        pager = driver.find_element(By.CSS_SELECTOR, "#gdasContentsArea div.pageing")
        links = pager.find_elements(By.CSS_SELECTOR, "a")
        for a in links:
            t = (a.text or "").strip()
            if t == "1":
                safe_click(a)
                sleep_smart(0.4, 0.8)
                break
    except NoSuchElementException:
        pass

def apply_gender_filter(gcode: str):  # 'F' or 'M'
    """
    ì„±ë³„ í•„í„° ì ìš© + ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ ì¬ë¡œë”© ëŒ€ê¸°.
    - label[for='sati_type5_1/2'] ë˜ëŠ” input[name='sati_type5'][value='F/M']
    - í´ë¦­ ì‹¤íŒ¨ ì‹œ JS ê°•ì œ ì²´í¬ + change ì´ë²¤íŠ¸
    - ì ìš©/ê²€ìƒ‰ ë²„íŠ¼ì„ ë‹¤ì–‘í•œ ì…€ë ‰í„°Â·í…ìŠ¤íŠ¸ë¡œ ì‹œë„
    """
    open_filter_panel()

    target_for = "sati_type5_1" if gcode == "F" else "sati_type5_2"
    val = "F" if gcode == "F" else "M"

    # 1) ë¼ë²¨/ì¸í’‹ ì°¾ê¸°
    label = driver.find_elements(By.CSS_SELECTOR, f"#filterDiv label[for='{target_for}']")
    inp = driver.find_elements(By.CSS_SELECTOR, f"#filterDiv input[name='sati_type5'][value='{val}']")
    if not (label or inp):
        # ë°±ì—…: í…ìŠ¤íŠ¸ ê¸°ë°˜
        txt = "ì—¬ì„±" if gcode == "F" else "ë‚¨ì„±"
        label = driver.find_elements(By.XPATH, f"//*[@id='filterDiv']//label[contains(., '{txt}')]")

    el = (label[0] if label else (inp[0] if inp else None))
    if not el:
        print(f"âš ï¸ ì„±ë³„ ë¼ë²¨/ì¸í’‹ì„ ëª» ì°¾ìŒ: {gcode}")
        return

    # 2) ë³´ì´ê²Œ ìŠ¤í¬ë¡¤
    try:
        driver.execute_script("arguments[0].scrollIntoView({block:'center'});", el)
        time.sleep(0.2)
    except Exception:
        pass

    # 3) ì„ íƒ(ë¼ë²¨ â†’ ì¸í’‹ â†’ JS ê°•ì œ)
    try:
        driver.execute_script("arguments[0].click();", el)
        time.sleep(0.2)
    except Exception:
        pass

    if inp:
        radio = inp[0]
        try:
            if not radio.is_selected():
                driver.execute_script("arguments[0].click();", radio)
                time.sleep(0.2)
            if not radio.is_selected():
                driver.execute_script("""
                    const r=arguments[0];
                    r.checked=true;
                    r.dispatchEvent(new Event('change',{bubbles:true}));
                """, radio)
                time.sleep(0.2)
        except Exception:
            pass

    # 4) ì ìš©/ê²€ìƒ‰ ë²„íŠ¼ ì—¬ëŸ¬ í›„ë³´ ì‹œë„
    apply_clicked = False
    apply_candidates = [
        "#filterDiv .btnArea .btnGreen",
        "#filterDiv .btn_area .btnGreen",
        "#filterDiv button.btnGreen",
        "#filterDiv .btn_confirm",
        "#filterDiv button[type='button'].btn_confirm",
        "#filterDiv .btn_srch",
        "#filterDiv .btn_search",
    ]
    for css in apply_candidates:
        try:
            b = driver.find_element(By.CSS_SELECTOR, css)
            if b and b.is_enabled():
                driver.execute_script("arguments[0].click();", b)
                apply_clicked = True
                break
        except Exception:
            continue
    if not apply_clicked:
        try:
            btns = driver.find_elements(By.XPATH, "//*[@id='filterDiv']//button[contains(., 'ì ìš©') or contains(., 'ê²€ìƒ‰')]")
            if btns:
                driver.execute_script("arguments[0].click();", btns[0])
                apply_clicked = True
        except Exception:
            pass

    # 5) ë¦¬ìŠ¤íŠ¸ ê°±ì‹  ëŒ€ê¸° + 1í˜ì´ì§€ ë³´ì •
    try:
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#gdasList")))
        sleep_smart(0.6, 1.0)
    except TimeoutException:
        pass
    ensure_first_review_page()

# =========================
# 7) ë¦¬ë·° í¬ë¡¤ë§(ì„±ë³„ë³„, ëê¹Œì§€)
# =========================
def crawl_reviews_for_product(product):
    all_reviews = []
    url = product["product_link"]

    driver.get(url)
    sleep_smart(1.2, 1.8)

    tab = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "#reviewInfo > a")))
    safe_click(tab)
    sleep_smart(0.8, 1.2)

    for gcode, glabel in [("F","ì—¬ì„±"), ("M","ë‚¨ì„±")]:
        try:
            apply_gender_filter(gcode)
        except Exception as e:
            print(f"âš ï¸ ì„±ë³„ í•„í„° ì ìš© ì‹¤íŒ¨({glabel}): {e}")
            continue

        page_no = 1
        SEEN = set()

        while True:
            try:
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#gdasList")))
            except TimeoutException:
                print(f"âŒ ë¦¬ë·° ëª©ë¡ ë¡œë”© ì‹¤íŒ¨ ({product['product_name']} / {glabel} / p.{page_no})")
                break

            items = driver.find_elements(By.CSS_SELECTOR, "#gdasList > li")

            for it in items:
                try:
                    if it.find_elements(By.CSS_SELECTOR, "div.info > div > p.info_user > a.id"):
                        customer_name = it.find_element(By.CSS_SELECTOR, "div.info > div > p.info_user > a.id").text.strip()
                    else:
                        customer_name = "Anonymous"

                    spans = it.find_elements(By.CSS_SELECTOR, "div.info > div > p.tag > span")
                    span_texts = [s.text.strip() for s in spans if s.text.strip()]
                    skin_type_val, skin_tone_val, skin_concerns_val = split_skin_tags(span_texts)

                    review_text = ""
                    if it.find_elements(By.CSS_SELECTOR, "div.review_cont > div.txt_inner"):
                        review_text = it.find_element(By.CSS_SELECTOR, "div.review_cont > div.txt_inner").text.strip()

                    date = "N/A"
                    if it.find_elements(By.CSS_SELECTOR, "div.review_cont > div.score_area > span.date"):
                        date = it.find_element(By.CSS_SELECTOR, "div.review_cont > div.score_area > span.date").text.strip()

                    rating_text, rating_val = "", ""
                    rt = it.find_elements(By.CSS_SELECTOR, "div.review_cont > div.score_area > span.review_point > span")
                    if rt:
                        rating_text = (rt[0].get_attribute("title") or rt[0].text or "").strip()
                        parsed = parse_rating_to_float(rating_text)
                        rating_val = parsed if parsed is not None else ""

                    sig = (customer_name, date, review_text, glabel)
                    if sig in SEEN:
                        continue
                    SEEN.add(sig)

                    all_reviews.append({
                        "customer_name": customer_name,
                        "skin_type": skin_type_val,
                        "skin_tone": skin_tone_val,
                        "skin_concerns": skin_concerns_val,
                        "review": review_text,
                        "date": date,
                        "rating_text": rating_text,
                        "rating": rating_val,
                        "gender": glabel,
                    })
                except (StaleElementReferenceException, Exception):
                    continue

            # ë‹¤ìŒ í˜ì´ì§€
            try:
                pager = driver.find_element(By.CSS_SELECTOR, "#gdasContentsArea div.pageing")
                links = pager.find_elements(By.CSS_SELECTOR, "a")
            except NoSuchElementException:
                break

            next_clicked = False
            for a in links:
                t = (a.text or "").strip()
                if t.isdigit():
                    try:
                        if int(t) == page_no + 1:
                            if safe_click(a):
                                sleep_smart(0.7, 1.1)
                                page_no += 1
                                next_clicked = True
                            break
                    except:
                        pass
            if not next_clicked:
                for a in links:
                    label = (a.get_attribute("aria-label") or a.text or "").strip()
                    if "ë‹¤ìŒ" in label or "next" in label.lower():
                        if safe_click(a):
                            sleep_smart(0.7, 1.1)
                            page_no += 1
                            next_clicked = True
                        break
            if not next_clicked:
                break

    return all_reviews

# =========================
# 8) ë©”ì¸
# =========================
try:
    print("â–¶ ìƒí’ˆ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘")
    products = crawl_product_list()

    if MAX_PRODUCTS is not None:
        products = products[START_AT: START_AT + MAX_PRODUCTS]
    elif START_AT:
        products = products[START_AT:]

    print(f"ì´ ìˆ˜ì§‘ ëŒ€ìƒ ìƒí’ˆ ìˆ˜: {len(products)}")
    write_product_list_csv(products, PRODUCT_LIST_CSV)
    print(f"ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ì €ì¥: {PRODUCT_LIST_CSV}")

    init_reviews_csv(CSV_PATH)

    for idx, product in enumerate(products, 1):
        print(f"\nğŸ” ({idx}/{len(products)}) ë¦¬ë·° í¬ë¡¤ë§: {product['product_name']}")
        try:
            rs = crawl_reviews_for_product(product)
        except (InvalidSessionIdException, WebDriverException) as e:
            print(f"âš ï¸ ë“œë¼ì´ë²„ ì˜¤ë¥˜ â†’ ì¬ìƒì„± í›„ ì¬ì‹œë„: {e}")
            recreate_driver()
            try:
                rs = crawl_reviews_for_product(product)
            except Exception as e2:
                print(f"âŒ ì¬ì‹œë„ ì‹¤íŒ¨: {e2}")
                continue
        append_reviews_to_csv(product, rs, CSV_PATH)
        print(f"  â†³ ìˆ˜ì§‘ ë¦¬ë·° ìˆ˜: {len(rs)}")
        sleep_smart(2.0, 3.5)

    print(f"\nâœ… ì™„ë£Œ! ë¦¬ë·° CSV ì €ì¥: {CSV_PATH}")

finally:
    try:
        driver.quit()
    except:
        pass
    print("ë¸Œë¼ìš°ì € ì¢…ë£Œ")
