{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 주요 특징:\n",
    "- 한국어 전용: 한국어의 특성을 고려한 감성사전\n",
    "- 품사별 분류: 명사, 동사, 형용사, 부사 등 품사별로 감성 단어 분류\n",
    "- 긍정/부정 레이블: 각 단어에 긍정(1) 또는 부정(0) 레이블 부여\n",
    "- 학술적 기반: 한국어 자연어처리 연구에서 널리 사용되는 표준 사전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1단계 : total_test_tokens.csv 생성 (평점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: (1094, 11)\n",
      "평점 분포:\n",
      "rating\n",
      "1      9\n",
      "2      9\n",
      "3     42\n",
      "4    131\n",
      "5    903\n",
      "Name: count, dtype: int64\n",
      "\n",
      "평점별 비율:\n",
      "rating\n",
      "1    0.008227\n",
      "2    0.008227\n",
      "3    0.038391\n",
      "4    0.119744\n",
      "5    0.825411\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1단계: 데이터 로드 및 분석\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv('/Users/Shared/최종선_교수님/Face_skin_disease/데이터 전처리/Ntoken_review.csv', encoding='utf-8')\n",
    "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
    "\n",
    "print(f\"데이터 크기: {df.shape}\")\n",
    "print(\"평점 분포:\")\n",
    "print(df['rating'].value_counts().sort_index())\n",
    "print(\"\\n평점별 비율:\")\n",
    "print(df['rating'].value_counts(normalize=True).sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 긍정 사전: 4868개\n",
      "기존 부정 사전: 9827개\n"
     ]
    }
   ],
   "source": [
    "# 기존 사전 로드\n",
    "with open('pos_pol_word.txt', 'r', encoding='utf-8') as f:\n",
    "    pos_words = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open('neg_pol_word.txt', 'r', encoding='utf-8') as f:\n",
    "    neg_words = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"기존 긍정 사전: {len(pos_words)}개\")\n",
    "print(f\"기존 부정 사전: {len(neg_words)}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 화장품 도메인 특화 감성사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "화장품 특화 긍정 사전: 80개\n",
      "화장품 특화 부정 사전: 81개\n",
      "✅ cosmetic_pos_words.txt, cosmetic_neg_words.txt 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# 2단계: 화장품 특화 감성사전 구축\n",
    "from konlpy.tag import Komoran\n",
    "from collections import Counter\n",
    "\n",
    "# 화장품 도메인 특화 감성사전 생성\n",
    "cosmetic_pos_words = [\n",
    "    # 보습/수분 관련\n",
    "    '보습', '수분', '촉촉', '윤기', '탄력', '부드럽', '순하', '진정', '완화',\n",
    "    '효과', '좋', '만족', '추천', '재구매', '사용감', '발림성', '흡수',\n",
    "    \n",
    "    # 긍정 표현\n",
    "    '최고', '완벽', '대만족', '훌륭', '훌륭하', '훌륭한', '훌륭하다', '훌륭해',\n",
    "    '좋아', '좋아요', '좋다', '좋은', '좋네', '좋네요', '좋아서', '좋았',\n",
    "    '만족', '만족하', '만족해', '만족해요', '만족스러', '만족스럽', '만족스러워',\n",
    "    '추천', '추천하', '추천해', '추천해요', '추천드리', '추천드려', '추천드려요',\n",
    "    \n",
    "    # 브랜드/제품 긍정\n",
    "    '신뢰', '믿음', '믿을', '믿고', '믿어', '믿어요', '믿는다', '믿어',\n",
    "    '품질', '고품질', '우수', '우수하', '우수한', '우수해', '우수해요',\n",
    "    \n",
    "    # 사용 경험 긍정\n",
    "    '편리', '편리하', '편리한', '편리해', '편리해요', '편리하게',\n",
    "    '쉽', '쉬워', '쉬워요', '쉬운', '쉽게', '쉽고',\n",
    "    '빠르', '빠른', '빠르게', '빠르고', '빨라', '빨라요'\n",
    "]\n",
    "\n",
    "cosmetic_neg_words = [\n",
    "    # 부정적 피부 반응\n",
    "    '자극', '알레르기', '트러블', '여드름', '붉어', '붉은', '붉게', '붉고',\n",
    "    '가려움', '가려워', '가려워요', '간지러', '간지러워', '간지러워요',\n",
    "    '부어', '부었', '부어요', '부었어요', '부종', '부으',\n",
    "    '따갑', '따가워', '따가워요', '따갑고', '따갑게',\n",
    "    \n",
    "    # # 제품 문제\n",
    "    # '끈적', '끈적해', '끈적해요', '끈적하고', '끈적하게', '끈적거리',\n",
    "    # '무겁', '무거워', '무거워요', '무거운', '무겁게', '무겁고',\n",
    "    # '기름지', '기름져', '기름져요', '기름진', '기름지게', '기름지고',\n",
    "    # '번들', '번들거리', '번들거려', '번들거려요', '번들거리고',\n",
    "    \n",
    "    # 부정 표현\n",
    "    '별로', '별로야', '별로예요', '별로네', '별로네요', '별로고',\n",
    "    '아쉽', '아쉬워', '아쉬워요', '아쉬운', '아쉽게', '아쉽고',\n",
    "    '실망', '실망하', '실망해', '실망해요', '실망스러', '실망스럽',\n",
    "    '후회', '후회하', '후회해', '후회해요', '후회돼', '후회돼요',\n",
    "    \n",
    "    # 효과 부족\n",
    "    '효과없', '효과없어', '효과없어요', '효과없고', '효과없게',\n",
    "    '변화없', '변화없어', '변화없어요', '변화없고', '변화없게',\n",
    "    '개선없', '개선없어', '개선없어요', '개선없고', '개선없게',\n",
    "    \n",
    "    # 가격/가성비 부정\n",
    "    '비싸', '비싸요', '비싼', '비싸고', '비싸게', '비싸서',\n",
    "    '아깝', '아까워', '아까워요', '아까운', '아깝게', '아깝고',\n",
    "    '돈아까', '돈아까워', '돈아까워요', '돈아까운', '돈아깝게'\n",
    "]\n",
    "\n",
    "# 사전 저장\n",
    "with open('cosmetic_pos_words.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in cosmetic_pos_words:\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "with open('cosmetic_neg_words.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in cosmetic_neg_words:\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(f\"화장품 특화 긍정 사전: {len(cosmetic_pos_words)}개\")\n",
    "print(f\"화장품 특화 부정 사전: {len(cosmetic_neg_words)}개\")\n",
    "print(\"✅ cosmetic_pos_words.txt, cosmetic_neg_words.txt 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 화장품 특화 감성사전 구축 ===\n",
      "화장품 특화 긍정사전: 28개 (1-gram), 7개 (2-gram), 0개 (3-gram)\n",
      "화장품 특화 부정사전: 18개 (1-gram), 13개 (2-gram), 0개 (3-gram)\n"
     ]
    }
   ],
   "source": [
    "# 기존 노트북의 2-3단계를 화장품 특화 사전으로 교체\n",
    "print(\"=== 화장품 특화 감성사전 구축 ===\")\n",
    "\n",
    "# 화장품 특화 사전 로드\n",
    "with open('cosmetic_pos_words.txt', 'r', encoding='utf-8') as f:\n",
    "    cosmetic_pos_words = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open('cosmetic_neg_words.txt', 'r', encoding='utf-8') as f:\n",
    "    cosmetic_neg_words = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Komoran으로 형태소 분석\n",
    "komoran = Komoran()\n",
    "tag_list = ['NNG', 'NNP', 'VV', 'VA', 'XR', 'MAG']\n",
    "\n",
    "# 긍정사전 정제\n",
    "pos_token = [komoran.pos(token) for token in cosmetic_pos_words]\n",
    "pos_token_list = [[token[0] for token in tokens if token[1] in tag_list] for tokens in pos_token]\n",
    "pos_token_list = [tokens for tokens in pos_token_list if len(tokens) > 0]\n",
    "\n",
    "# 중복 제거\n",
    "pos_unique_dict = []\n",
    "for t in pos_token_list:\n",
    "    if t not in pos_unique_dict:\n",
    "        pos_unique_dict.append(t)\n",
    "\n",
    "# 부정사전 정제\n",
    "neg_token = [komoran.pos(token) for token in cosmetic_neg_words]\n",
    "neg_token_list = [[token[0] for token in tokens if token[1] in tag_list] for tokens in neg_token]\n",
    "neg_token_list = [tokens for tokens in neg_token_list if len(tokens) > 0]\n",
    "\n",
    "neg_unique_dict = []\n",
    "for t in neg_token_list:\n",
    "    if t not in neg_unique_dict:\n",
    "        neg_unique_dict.append(t)\n",
    "\n",
    "# 1-gram, 2-gram, 3-gram 분류\n",
    "pos_dict1, pos_dict2, pos_dict3 = [], [], []\n",
    "for t in pos_unique_dict:\n",
    "    if len(t) == 1:\n",
    "        pos_dict1.append(t[0])\n",
    "    elif len(t) == 2:\n",
    "        pos_dict2.append(t)\n",
    "    elif len(t) == 3:\n",
    "        pos_dict3.append(t)\n",
    "\n",
    "neg_dict1, neg_dict2, neg_dict3 = [], [], []\n",
    "for t in neg_unique_dict:\n",
    "    if len(t) == 1:\n",
    "        neg_dict1.append(t[0])\n",
    "    elif len(t) == 2:\n",
    "        neg_dict2.append(t)\n",
    "    elif len(t) == 3:\n",
    "        neg_dict3.append(t)\n",
    "\n",
    "print(f\"화장품 특화 긍정사전: {len(pos_dict1)}개 (1-gram), {len(pos_dict2)}개 (2-gram), {len(pos_dict3)}개 (3-gram)\")\n",
    "print(f\"화장품 특화 부정사전: {len(neg_dict1)}개 (1-gram), {len(neg_dict2)}개 (2-gram), {len(neg_dict3)}개 (3-gram)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 결합된 감성사전 생성 ===\n",
      "기존 KNU 긍정 사전: 4868개\n",
      "화장품 긍정 사전: 80개\n",
      "결합된 긍정 사전: 4927개\n",
      "  → 추가된 단어: 59개\n",
      "\n",
      "기존 KNU 부정 사전: 9827개\n",
      "화장품 부정 사전: 81개\n",
      "결합된 부정 사전: 9901개\n",
      "  → 추가된 단어: 74개\n",
      "\n",
      "✅ 결합된 사전 저장 완료!\n",
      "  - combined_pos_words.txt: 4927개\n",
      "  - combined_neg_words.txt: 9901개\n"
     ]
    }
   ],
   "source": [
    "# 기존 KNU 사전 + 화장품 특화 사전 결합\n",
    "def create_combined_sentiment_dictionary():\n",
    "    \"\"\"기존 KNU 사전과 화장품 특화 사전을 결합\"\"\"\n",
    "    print(\"=== 결합된 감성사전 생성 ===\")\n",
    "    \n",
    "    # 1. 기존 KNU 사전 로드\n",
    "    with open('pos_pol_word.txt', 'r', encoding='utf-8') as f:\n",
    "        knu_pos_words = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    with open('neg_pol_word.txt', 'r', encoding='utf-8') as f:\n",
    "        knu_neg_words = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # 2. 화장품 특화 사전 로드\n",
    "    with open('cosmetic_pos_words.txt', 'r', encoding='utf-8') as f:\n",
    "        cosmetic_pos_words = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    with open('cosmetic_neg_words.txt', 'r', encoding='utf-8') as f:\n",
    "        cosmetic_neg_words = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # 3. 사전 결합 (중복 제거)\n",
    "    combined_pos_words = list(set(knu_pos_words + cosmetic_pos_words))\n",
    "    combined_neg_words = list(set(knu_neg_words + cosmetic_neg_words))\n",
    "    \n",
    "    print(f\"기존 KNU 긍정 사전: {len(knu_pos_words)}개\")\n",
    "    print(f\"화장품 긍정 사전: {len(cosmetic_pos_words)}개\")\n",
    "    print(f\"결합된 긍정 사전: {len(combined_pos_words)}개\")\n",
    "    print(f\"  → 추가된 단어: {len(combined_pos_words) - len(knu_pos_words)}개\")\n",
    "    \n",
    "    print(f\"\\n기존 KNU 부정 사전: {len(knu_neg_words)}개\")\n",
    "    print(f\"화장품 부정 사전: {len(cosmetic_neg_words)}개\")\n",
    "    print(f\"결합된 부정 사전: {len(combined_neg_words)}개\")\n",
    "    print(f\"  → 추가된 단어: {len(combined_neg_words) - len(knu_neg_words)}개\")\n",
    "    \n",
    "    # 4. 결합된 사전 저장\n",
    "    with open('combined_pos_words.txt', 'w', encoding='utf-8') as f:\n",
    "        for word in combined_pos_words:\n",
    "            f.write(word + '\\n')\n",
    "    \n",
    "    with open('combined_neg_words.txt', 'w', encoding='utf-8') as f:\n",
    "        for word in combined_neg_words:\n",
    "            f.write(word + '\\n')\n",
    "    \n",
    "    print(f\"\\n✅ 결합된 사전 저장 완료!\")\n",
    "    print(f\"  - combined_pos_words.txt: {len(combined_pos_words)}개\")\n",
    "    print(f\"  - combined_neg_words.txt: {len(combined_neg_words)}개\")\n",
    "    \n",
    "    return combined_pos_words, combined_neg_words\n",
    "\n",
    "# 결합된 사전 생성\n",
    "combined_pos, combined_neg = create_combined_sentiment_dictionary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 결합된 사전으로 total_test_tokens3.csv 생성\n",
    "# def create_total_test_tokens3_with_combined_dict():\n",
    "#     \"\"\"결합된 사전을 이용한 total_test_tokens3.csv 생성\"\"\"\n",
    "#     print(\"=== 결합된 사전으로 total_test_tokens3.csv 생성 ===\")\n",
    "    \n",
    "#     # 1. 데이터 로드\n",
    "#     df = pd.read_csv('/Users/Shared/최종선_교수님/Face_skin_disease/데이터 전처리/Ntoken_review.csv', encoding='utf-8')\n",
    "#     print(f\"원본 데이터 크기: {df.shape}\")\n",
    "    \n",
    "#     # 2. 결합된 사전 로드\n",
    "#     with open('combined_pos_words.txt', 'r', encoding='utf-8') as f:\n",
    "#         combined_pos_words = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "#     with open('combined_neg_words.txt', 'r', encoding='utf-8') as f:\n",
    "#         combined_neg_words = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "#     print(f\"결합된 긍정 사전: {len(combined_pos_words)}개\")\n",
    "#     print(f\"결합된 부정 사전: {len(combined_neg_words)}개\")\n",
    "    \n",
    "#     # 3. Komoran 초기화\n",
    "#     komoran = Komoran()\n",
    "#     tag_list = ['NNG', 'NNP', 'VV', 'VA', 'XR', 'MAG']\n",
    "    \n",
    "#     # 4. 결합된 사전을 형태소 분석하여 정제\n",
    "#     def process_dictionary(words, komoran, tag_list):\n",
    "#         \"\"\"사전을 형태소 분석하여 정제\"\"\"\n",
    "#         processed_words = []\n",
    "#         for word in words:\n",
    "#             try:\n",
    "#                 pos_result = komoran.pos(word)\n",
    "#                 filtered_tokens = [token[0] for token in pos_result if token[1] in tag_list]\n",
    "#                 if filtered_tokens:\n",
    "#                     processed_words.extend(filtered_tokens)\n",
    "#             except:\n",
    "#                 continue\n",
    "#         return list(set(processed_words))  # 중복 제거\n",
    "    \n",
    "#     # 결합된 사전 정제\n",
    "#     combined_pos_processed = process_dictionary(combined_pos_words, komoran, tag_list)\n",
    "#     combined_neg_processed = process_dictionary(combined_neg_words, komoran, tag_list)\n",
    "    \n",
    "#     print(f\"정제된 결합 긍정 사전: {len(combined_pos_processed)}개\")\n",
    "#     print(f\"정제된 결합 부정 사전: {len(combined_neg_processed)}개\")\n",
    "    \n",
    "#     # 5. 리뷰 텍스트를 형태소 분석하고 감성 점수 계산\n",
    "#     def calculate_sentiment_score(text, pos_words, neg_words, komoran, tag_list):\n",
    "#         \"\"\"리뷰 텍스트의 감성 점수 계산\"\"\"\n",
    "#         if pd.isna(text):\n",
    "#             return 0.0\n",
    "        \n",
    "#         try:\n",
    "#             # 형태소 분석\n",
    "#             pos_result = komoran.pos(str(text))\n",
    "#             tokens = [token[0] for token in pos_result if token[1] in tag_list]\n",
    "            \n",
    "#             # 감성 단어 카운트\n",
    "#             pos_count = sum(1 for token in tokens if token in pos_words)\n",
    "#             neg_count = sum(1 for token in tokens if token in neg_words)\n",
    "            \n",
    "#             # 감성 점수 계산 (긍정 단어 - 부정 단어)\n",
    "#             total_words = len(tokens)\n",
    "#             if total_words == 0:\n",
    "#                 return 0.0\n",
    "            \n",
    "#             sentiment_score = (pos_count - neg_count) / total_words\n",
    "#             return sentiment_score\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"오류 발생: {e}\")\n",
    "#             return 0.0\n",
    "    \n",
    "#     # 6. 각 리뷰에 대해 감성 점수 계산\n",
    "#     print(\"리뷰 감성 분석 중...\")\n",
    "#     sentiment_scores = []\n",
    "    \n",
    "#     for idx, row in df.iterrows():\n",
    "#         if idx % 100 == 0:\n",
    "#             print(f\"진행률: {idx}/{len(df)} ({idx/len(df)*100:.1f}%)\")\n",
    "        \n",
    "#         score = calculate_sentiment_score(\n",
    "#             row['review'], \n",
    "#             combined_pos_processed, \n",
    "#             combined_neg_processed, \n",
    "#             komoran, \n",
    "#             tag_list\n",
    "#         )\n",
    "#         sentiment_scores.append(score)\n",
    "    \n",
    "#     # 7. 결과 데이터프레임 생성\n",
    "#     result_df = pd.DataFrame({\n",
    "#         'sen': df['review'].tolist(),\n",
    "#         'pos_neg': sentiment_scores\n",
    "#     })\n",
    "    \n",
    "#     # 8. 감성 점수 분포 확인\n",
    "#     print(\"\\n=== 감성 점수 분포 ===\")\n",
    "#     print(f\"평균 감성 점수: {np.mean(sentiment_scores):.4f}\")\n",
    "#     print(f\"표준편차: {np.std(sentiment_scores):.4f}\")\n",
    "#     print(f\"최솟값: {np.min(sentiment_scores):.4f}\")\n",
    "#     print(f\"최댓값: {np.max(sentiment_scores):.4f}\")\n",
    "    \n",
    "#     # 감성 점수별 분포\n",
    "#     positive_count = sum(1 for score in sentiment_scores if score > 0)\n",
    "#     negative_count = sum(1 for score in sentiment_scores if score < 0)\n",
    "#     neutral_count = sum(1 for score in sentiment_scores if score == 0)\n",
    "    \n",
    "#     print(f\"\\n긍정 (점수 > 0): {positive_count}개 ({positive_count/len(sentiment_scores)*100:.1f}%)\")\n",
    "#     print(f\"부정 (점수 < 0): {negative_count}개 ({negative_count/len(sentiment_scores)*100:.1f}%)\")\n",
    "#     print(f\"중립 (점수 = 0): {neutral_count}개 ({neutral_count/len(sentiment_scores)*100:.1f}%)\")\n",
    "    \n",
    "#     # 9. CSV 파일 저장\n",
    "#     output_file = 'total_test_tokens3.csv'\n",
    "#     result_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "#     print(f\"\\n✅ {output_file} 저장 완료!\")\n",
    "#     print(f\"저장된 데이터 크기: {result_df.shape}\")\n",
    "    \n",
    "#     return result_df\n",
    "\n",
    "# # 실행\n",
    "# total_test_tokens3 = create_total_test_tokens3_with_combined_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 세 가지 사전 성능 비교 ===\n",
      "\n",
      "1. 기존 KNU 사전 성능:\n",
      "  사전 크기: 긍정 1716개, 부정 2980개\n",
      "  정확도: 0.425\n",
      "\n",
      "2. 화장품 특화 사전 성능:\n",
      "  사전 크기: 긍정 35개, 부정 31개\n",
      "  정확도: 0.751\n",
      "\n",
      "3. 결합된 사전 성능:\n",
      "  사전 크기: 긍정 1727개, 부정 2994개\n",
      "  정확도: 0.518\n",
      "\n",
      "==================================================\n",
      "📊 성능 비교 요약\n",
      "==================================================\n",
      "KNU         : 0.425\n",
      "화장품 특화      : 0.751\n",
      "결합된 사전      : 0.518\n",
      "\n",
      "🏆 최고 성능: 화장품 특화 (0.751)\n"
     ]
    }
   ],
   "source": [
    "# 세 가지 사전 성능 비교 분석\n",
    "def compare_sentiment_dictionaries():\n",
    "    \"\"\"세 가지 사전의 성능 비교\"\"\"\n",
    "    print(\"=== 세 가지 사전 성능 비교 ===\")\n",
    "    \n",
    "    # 1. 데이터 로드\n",
    "    df = pd.read_csv('/Users/Shared/최종선_교수님/Face_skin_disease/데이터 전처리/Ntoken_review.csv', encoding='utf-8')\n",
    "    \n",
    "    # 2. 실제 감성 레이블 생성 (평점 기반)\n",
    "    df['actual_sentiment'] = df['rating'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "    \n",
    "    # 3. 각 사전별 성능 평가\n",
    "    results = {}\n",
    "    \n",
    "    # 3-1. 기존 KNU 사전\n",
    "    print(\"\\n1. 기존 KNU 사전 성능:\")\n",
    "    with open('pos_pol_word.txt', 'r', encoding='utf-8') as f:\n",
    "        knu_pos = [line.strip() for line in f.readlines()]\n",
    "    with open('neg_pol_word.txt', 'r', encoding='utf-8') as f:\n",
    "        knu_neg = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    knu_accuracy = evaluate_dictionary_performance(df, knu_pos, knu_neg, \"KNU\")\n",
    "    results['KNU'] = knu_accuracy\n",
    "    \n",
    "    # 3-2. 화장품 특화 사전\n",
    "    print(\"\\n2. 화장품 특화 사전 성능:\")\n",
    "    with open('cosmetic_pos_words.txt', 'r', encoding='utf-8') as f:\n",
    "        cosmetic_pos = [line.strip() for line in f.readlines()]\n",
    "    with open('cosmetic_neg_words.txt', 'r', encoding='utf-8') as f:\n",
    "        cosmetic_neg = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    cosmetic_accuracy = evaluate_dictionary_performance(df, cosmetic_pos, cosmetic_neg, \"화장품 특화\")\n",
    "    results['화장품 특화'] = cosmetic_accuracy\n",
    "    \n",
    "    # 3-3. 결합된 사전\n",
    "    print(\"\\n3. 결합된 사전 성능:\")\n",
    "    with open('combined_pos_words.txt', 'r', encoding='utf-8') as f:\n",
    "        combined_pos = [line.strip() for line in f.readlines()]\n",
    "    with open('combined_neg_words.txt', 'r', encoding='utf-8') as f:\n",
    "        combined_neg = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    combined_accuracy = evaluate_dictionary_performance(df, combined_pos, combined_neg, \"결합된 사전\")\n",
    "    results['결합된 사전'] = combined_accuracy\n",
    "    \n",
    "    # 4. 결과 요약\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📊 성능 비교 요약\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for name, accuracy in results.items():\n",
    "        print(f\"{name:12}: {accuracy:.3f}\")\n",
    "    \n",
    "    # 최고 성능 사전 찾기\n",
    "    best_dict = max(results, key=results.get)\n",
    "    print(f\"\\n🏆 최고 성능: {best_dict} ({results[best_dict]:.3f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_dictionary_performance(df, pos_words, neg_words, dict_name):\n",
    "    \"\"\"사전 성능 평가\"\"\"\n",
    "    komoran = Komoran()\n",
    "    tag_list = ['NNG', 'NNP', 'VV', 'VA', 'XR', 'MAG']\n",
    "    \n",
    "    # 사전 정제\n",
    "    def process_dictionary(words, komoran, tag_list):\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                pos_result = komoran.pos(word)\n",
    "                filtered_tokens = [token[0] for token in pos_result if token[1] in tag_list]\n",
    "                if filtered_tokens:\n",
    "                    processed_words.extend(filtered_tokens)\n",
    "            except:\n",
    "                continue\n",
    "        return list(set(processed_words))\n",
    "    \n",
    "    pos_processed = process_dictionary(pos_words, komoran, tag_list)\n",
    "    neg_processed = process_dictionary(neg_words, komoran, tag_list)\n",
    "    \n",
    "    # 감성 분석\n",
    "    def simple_sentiment_analysis(text, pos_words, neg_words):\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            pos_result = komoran.pos(str(text))\n",
    "            tokens = [token[0] for token in pos_result if token[1] in tag_list]\n",
    "            \n",
    "            pos_count = sum(1 for token in tokens if token in pos_words)\n",
    "            neg_count = sum(1 for token in tokens if token in neg_words)\n",
    "            \n",
    "            if pos_count > neg_count:\n",
    "                return 1\n",
    "            elif neg_count > pos_count:\n",
    "                return 0\n",
    "            else:\n",
    "                return 0.5\n",
    "        except:\n",
    "            return 0.5\n",
    "    \n",
    "    # 예측 수행\n",
    "    df['predicted'] = df['review'].apply(\n",
    "        lambda x: simple_sentiment_analysis(x, pos_processed, neg_processed)\n",
    "    )\n",
    "    \n",
    "    # 정확도 계산\n",
    "    accuracy = (df['predicted'] == df['actual_sentiment']).mean()\n",
    "    \n",
    "    print(f\"  사전 크기: 긍정 {len(pos_processed)}개, 부정 {len(neg_processed)}개\")\n",
    "    print(f\"  정확도: {accuracy:.3f}\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# 성능 비교 실행\n",
    "comparison_results = compare_sentiment_dictionaries()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
