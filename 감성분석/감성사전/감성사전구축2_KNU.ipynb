{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ì£¼ìš” íŠ¹ì§•:\n",
    "- í•œêµ­ì–´ ì „ìš©: í•œêµ­ì–´ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ ê°ì„±ì‚¬ì „\n",
    "- í’ˆì‚¬ë³„ ë¶„ë¥˜: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ë¶€ì‚¬ ë“± í’ˆì‚¬ë³„ë¡œ ê°ì„± ë‹¨ì–´ ë¶„ë¥˜\n",
    "- ê¸ì •/ë¶€ì • ë ˆì´ë¸”: ê° ë‹¨ì–´ì— ê¸ì •(1) ë˜ëŠ” ë¶€ì •(0) ë ˆì´ë¸” ë¶€ì—¬\n",
    "- í•™ìˆ ì  ê¸°ë°˜: í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬ ì—°êµ¬ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” í‘œì¤€ ì‚¬ì „"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ë‹¨ê³„ : total_test_tokens.csv ìƒì„± (í‰ì )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: (1094, 11)\n",
      "í‰ì  ë¶„í¬:\n",
      "rating\n",
      "1      9\n",
      "2      9\n",
      "3     42\n",
      "4    131\n",
      "5    903\n",
      "Name: count, dtype: int64\n",
      "\n",
      "í‰ì ë³„ ë¹„ìœ¨:\n",
      "rating\n",
      "1    0.008227\n",
      "2    0.008227\n",
      "3    0.038391\n",
      "4    0.119744\n",
      "5    0.825411\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° ë¶„ì„\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv('/Users/Shared/ìµœì¢…ì„ _êµìˆ˜ë‹˜/Face_skin_disease/ë°ì´í„° ì „ì²˜ë¦¬/Ntoken_review.csv', encoding='utf-8')\n",
    "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
    "\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
    "print(\"í‰ì  ë¶„í¬:\")\n",
    "print(df['rating'].value_counts().sort_index())\n",
    "print(\"\\ní‰ì ë³„ ë¹„ìœ¨:\")\n",
    "print(df['rating'].value_counts(normalize=True).sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°ì¡´ ê¸ì • ì‚¬ì „: 4868ê°œ\n",
      "ê¸°ì¡´ ë¶€ì • ì‚¬ì „: 9827ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ì¡´ ì‚¬ì „ ë¡œë“œ\n",
    "with open('pos_pol_word.txt', 'r', encoding='utf-8') as f:\n",
    "    pos_words = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open('neg_pol_word.txt', 'r', encoding='utf-8') as f:\n",
    "    neg_words = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"ê¸°ì¡´ ê¸ì • ì‚¬ì „: {len(pos_words)}ê°œ\")\n",
    "print(f\"ê¸°ì¡´ ë¶€ì • ì‚¬ì „: {len(neg_words)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í™”ì¥í’ˆ ë„ë©”ì¸ íŠ¹í™” ê°ì„±ì‚¬ì „ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í™”ì¥í’ˆ íŠ¹í™” ê¸ì • ì‚¬ì „: 80ê°œ\n",
      "í™”ì¥í’ˆ íŠ¹í™” ë¶€ì • ì‚¬ì „: 81ê°œ\n",
      "âœ… cosmetic_pos_words.txt, cosmetic_neg_words.txt ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# 2ë‹¨ê³„: í™”ì¥í’ˆ íŠ¹í™” ê°ì„±ì‚¬ì „ êµ¬ì¶•\n",
    "from konlpy.tag import Komoran\n",
    "from collections import Counter\n",
    "\n",
    "# í™”ì¥í’ˆ ë„ë©”ì¸ íŠ¹í™” ê°ì„±ì‚¬ì „ ìƒì„±\n",
    "cosmetic_pos_words = [\n",
    "    # ë³´ìŠµ/ìˆ˜ë¶„ ê´€ë ¨\n",
    "    'ë³´ìŠµ', 'ìˆ˜ë¶„', 'ì´‰ì´‰', 'ìœ¤ê¸°', 'íƒ„ë ¥', 'ë¶€ë“œëŸ½', 'ìˆœí•˜', 'ì§„ì •', 'ì™„í™”',\n",
    "    'íš¨ê³¼', 'ì¢‹', 'ë§Œì¡±', 'ì¶”ì²œ', 'ì¬êµ¬ë§¤', 'ì‚¬ìš©ê°', 'ë°œë¦¼ì„±', 'í¡ìˆ˜',\n",
    "    \n",
    "    # ê¸ì • í‘œí˜„\n",
    "    'ìµœê³ ', 'ì™„ë²½', 'ëŒ€ë§Œì¡±', 'í›Œë¥­', 'í›Œë¥­í•˜', 'í›Œë¥­í•œ', 'í›Œë¥­í•˜ë‹¤', 'í›Œë¥­í•´',\n",
    "    'ì¢‹ì•„', 'ì¢‹ì•„ìš”', 'ì¢‹ë‹¤', 'ì¢‹ì€', 'ì¢‹ë„¤', 'ì¢‹ë„¤ìš”', 'ì¢‹ì•„ì„œ', 'ì¢‹ì•˜',\n",
    "    'ë§Œì¡±', 'ë§Œì¡±í•˜', 'ë§Œì¡±í•´', 'ë§Œì¡±í•´ìš”', 'ë§Œì¡±ìŠ¤ëŸ¬', 'ë§Œì¡±ìŠ¤ëŸ½', 'ë§Œì¡±ìŠ¤ëŸ¬ì›Œ',\n",
    "    'ì¶”ì²œ', 'ì¶”ì²œí•˜', 'ì¶”ì²œí•´', 'ì¶”ì²œí•´ìš”', 'ì¶”ì²œë“œë¦¬', 'ì¶”ì²œë“œë ¤', 'ì¶”ì²œë“œë ¤ìš”',\n",
    "    \n",
    "    # ë¸Œëœë“œ/ì œí’ˆ ê¸ì •\n",
    "    'ì‹ ë¢°', 'ë¯¿ìŒ', 'ë¯¿ì„', 'ë¯¿ê³ ', 'ë¯¿ì–´', 'ë¯¿ì–´ìš”', 'ë¯¿ëŠ”ë‹¤', 'ë¯¿ì–´',\n",
    "    'í’ˆì§ˆ', 'ê³ í’ˆì§ˆ', 'ìš°ìˆ˜', 'ìš°ìˆ˜í•˜', 'ìš°ìˆ˜í•œ', 'ìš°ìˆ˜í•´', 'ìš°ìˆ˜í•´ìš”',\n",
    "    \n",
    "    # ì‚¬ìš© ê²½í—˜ ê¸ì •\n",
    "    'í¸ë¦¬', 'í¸ë¦¬í•˜', 'í¸ë¦¬í•œ', 'í¸ë¦¬í•´', 'í¸ë¦¬í•´ìš”', 'í¸ë¦¬í•˜ê²Œ',\n",
    "    'ì‰½', 'ì‰¬ì›Œ', 'ì‰¬ì›Œìš”', 'ì‰¬ìš´', 'ì‰½ê²Œ', 'ì‰½ê³ ',\n",
    "    'ë¹ ë¥´', 'ë¹ ë¥¸', 'ë¹ ë¥´ê²Œ', 'ë¹ ë¥´ê³ ', 'ë¹¨ë¼', 'ë¹¨ë¼ìš”'\n",
    "]\n",
    "\n",
    "cosmetic_neg_words = [\n",
    "    # ë¶€ì •ì  í”¼ë¶€ ë°˜ì‘\n",
    "    'ìê·¹', 'ì•Œë ˆë¥´ê¸°', 'íŠ¸ëŸ¬ë¸”', 'ì—¬ë“œë¦„', 'ë¶‰ì–´', 'ë¶‰ì€', 'ë¶‰ê²Œ', 'ë¶‰ê³ ',\n",
    "    'ê°€ë ¤ì›€', 'ê°€ë ¤ì›Œ', 'ê°€ë ¤ì›Œìš”', 'ê°„ì§€ëŸ¬', 'ê°„ì§€ëŸ¬ì›Œ', 'ê°„ì§€ëŸ¬ì›Œìš”',\n",
    "    'ë¶€ì–´', 'ë¶€ì—ˆ', 'ë¶€ì–´ìš”', 'ë¶€ì—ˆì–´ìš”', 'ë¶€ì¢…', 'ë¶€ìœ¼',\n",
    "    'ë”°ê°‘', 'ë”°ê°€ì›Œ', 'ë”°ê°€ì›Œìš”', 'ë”°ê°‘ê³ ', 'ë”°ê°‘ê²Œ',\n",
    "    \n",
    "    # # ì œí’ˆ ë¬¸ì œ\n",
    "    # 'ëˆì ', 'ëˆì í•´', 'ëˆì í•´ìš”', 'ëˆì í•˜ê³ ', 'ëˆì í•˜ê²Œ', 'ëˆì ê±°ë¦¬',\n",
    "    # 'ë¬´ê²', 'ë¬´ê±°ì›Œ', 'ë¬´ê±°ì›Œìš”', 'ë¬´ê±°ìš´', 'ë¬´ê²ê²Œ', 'ë¬´ê²ê³ ',\n",
    "    # 'ê¸°ë¦„ì§€', 'ê¸°ë¦„ì ¸', 'ê¸°ë¦„ì ¸ìš”', 'ê¸°ë¦„ì§„', 'ê¸°ë¦„ì§€ê²Œ', 'ê¸°ë¦„ì§€ê³ ',\n",
    "    # 'ë²ˆë“¤', 'ë²ˆë“¤ê±°ë¦¬', 'ë²ˆë“¤ê±°ë ¤', 'ë²ˆë“¤ê±°ë ¤ìš”', 'ë²ˆë“¤ê±°ë¦¬ê³ ',\n",
    "    \n",
    "    # ë¶€ì • í‘œí˜„\n",
    "    'ë³„ë¡œ', 'ë³„ë¡œì•¼', 'ë³„ë¡œì˜ˆìš”', 'ë³„ë¡œë„¤', 'ë³„ë¡œë„¤ìš”', 'ë³„ë¡œê³ ',\n",
    "    'ì•„ì‰½', 'ì•„ì‰¬ì›Œ', 'ì•„ì‰¬ì›Œìš”', 'ì•„ì‰¬ìš´', 'ì•„ì‰½ê²Œ', 'ì•„ì‰½ê³ ',\n",
    "    'ì‹¤ë§', 'ì‹¤ë§í•˜', 'ì‹¤ë§í•´', 'ì‹¤ë§í•´ìš”', 'ì‹¤ë§ìŠ¤ëŸ¬', 'ì‹¤ë§ìŠ¤ëŸ½',\n",
    "    'í›„íšŒ', 'í›„íšŒí•˜', 'í›„íšŒí•´', 'í›„íšŒí•´ìš”', 'í›„íšŒë¼', 'í›„íšŒë¼ìš”',\n",
    "    \n",
    "    # íš¨ê³¼ ë¶€ì¡±\n",
    "    'íš¨ê³¼ì—†', 'íš¨ê³¼ì—†ì–´', 'íš¨ê³¼ì—†ì–´ìš”', 'íš¨ê³¼ì—†ê³ ', 'íš¨ê³¼ì—†ê²Œ',\n",
    "    'ë³€í™”ì—†', 'ë³€í™”ì—†ì–´', 'ë³€í™”ì—†ì–´ìš”', 'ë³€í™”ì—†ê³ ', 'ë³€í™”ì—†ê²Œ',\n",
    "    'ê°œì„ ì—†', 'ê°œì„ ì—†ì–´', 'ê°œì„ ì—†ì–´ìš”', 'ê°œì„ ì—†ê³ ', 'ê°œì„ ì—†ê²Œ',\n",
    "    \n",
    "    # ê°€ê²©/ê°€ì„±ë¹„ ë¶€ì •\n",
    "    'ë¹„ì‹¸', 'ë¹„ì‹¸ìš”', 'ë¹„ì‹¼', 'ë¹„ì‹¸ê³ ', 'ë¹„ì‹¸ê²Œ', 'ë¹„ì‹¸ì„œ',\n",
    "    'ì•„ê¹', 'ì•„ê¹Œì›Œ', 'ì•„ê¹Œì›Œìš”', 'ì•„ê¹Œìš´', 'ì•„ê¹ê²Œ', 'ì•„ê¹ê³ ',\n",
    "    'ëˆì•„ê¹Œ', 'ëˆì•„ê¹Œì›Œ', 'ëˆì•„ê¹Œì›Œìš”', 'ëˆì•„ê¹Œìš´', 'ëˆì•„ê¹ê²Œ'\n",
    "]\n",
    "\n",
    "# ì‚¬ì „ ì €ì¥\n",
    "with open('cosmetic_pos_words.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in cosmetic_pos_words:\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "with open('cosmetic_neg_words.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in cosmetic_neg_words:\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(f\"í™”ì¥í’ˆ íŠ¹í™” ê¸ì • ì‚¬ì „: {len(cosmetic_pos_words)}ê°œ\")\n",
    "print(f\"í™”ì¥í’ˆ íŠ¹í™” ë¶€ì • ì‚¬ì „: {len(cosmetic_neg_words)}ê°œ\")\n",
    "print(\"âœ… cosmetic_pos_words.txt, cosmetic_neg_words.txt ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== í™”ì¥í’ˆ íŠ¹í™” ê°ì„±ì‚¬ì „ êµ¬ì¶• ===\n",
      "í™”ì¥í’ˆ íŠ¹í™” ê¸ì •ì‚¬ì „: 28ê°œ (1-gram), 7ê°œ (2-gram), 0ê°œ (3-gram)\n",
      "í™”ì¥í’ˆ íŠ¹í™” ë¶€ì •ì‚¬ì „: 18ê°œ (1-gram), 13ê°œ (2-gram), 0ê°œ (3-gram)\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ì¡´ ë…¸íŠ¸ë¶ì˜ 2-3ë‹¨ê³„ë¥¼ í™”ì¥í’ˆ íŠ¹í™” ì‚¬ì „ìœ¼ë¡œ êµì²´\n",
    "print(\"=== í™”ì¥í’ˆ íŠ¹í™” ê°ì„±ì‚¬ì „ êµ¬ì¶• ===\")\n",
    "\n",
    "# í™”ì¥í’ˆ íŠ¹í™” ì‚¬ì „ ë¡œë“œ\n",
    "with open('cosmetic_pos_words.txt', 'r', encoding='utf-8') as f:\n",
    "    cosmetic_pos_words = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open('cosmetic_neg_words.txt', 'r', encoding='utf-8') as f:\n",
    "    cosmetic_neg_words = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Komoranìœ¼ë¡œ í˜•íƒœì†Œ ë¶„ì„\n",
    "komoran = Komoran()\n",
    "tag_list = ['NNG', 'NNP', 'VV', 'VA', 'XR', 'MAG']\n",
    "\n",
    "# ê¸ì •ì‚¬ì „ ì •ì œ\n",
    "pos_token = [komoran.pos(token) for token in cosmetic_pos_words]\n",
    "pos_token_list = [[token[0] for token in tokens if token[1] in tag_list] for tokens in pos_token]\n",
    "pos_token_list = [tokens for tokens in pos_token_list if len(tokens) > 0]\n",
    "\n",
    "# ì¤‘ë³µ ì œê±°\n",
    "pos_unique_dict = []\n",
    "for t in pos_token_list:\n",
    "    if t not in pos_unique_dict:\n",
    "        pos_unique_dict.append(t)\n",
    "\n",
    "# ë¶€ì •ì‚¬ì „ ì •ì œ\n",
    "neg_token = [komoran.pos(token) for token in cosmetic_neg_words]\n",
    "neg_token_list = [[token[0] for token in tokens if token[1] in tag_list] for tokens in neg_token]\n",
    "neg_token_list = [tokens for tokens in neg_token_list if len(tokens) > 0]\n",
    "\n",
    "neg_unique_dict = []\n",
    "for t in neg_token_list:\n",
    "    if t not in neg_unique_dict:\n",
    "        neg_unique_dict.append(t)\n",
    "\n",
    "# 1-gram, 2-gram, 3-gram ë¶„ë¥˜\n",
    "pos_dict1, pos_dict2, pos_dict3 = [], [], []\n",
    "for t in pos_unique_dict:\n",
    "    if len(t) == 1:\n",
    "        pos_dict1.append(t[0])\n",
    "    elif len(t) == 2:\n",
    "        pos_dict2.append(t)\n",
    "    elif len(t) == 3:\n",
    "        pos_dict3.append(t)\n",
    "\n",
    "neg_dict1, neg_dict2, neg_dict3 = [], [], []\n",
    "for t in neg_unique_dict:\n",
    "    if len(t) == 1:\n",
    "        neg_dict1.append(t[0])\n",
    "    elif len(t) == 2:\n",
    "        neg_dict2.append(t)\n",
    "    elif len(t) == 3:\n",
    "        neg_dict3.append(t)\n",
    "\n",
    "print(f\"í™”ì¥í’ˆ íŠ¹í™” ê¸ì •ì‚¬ì „: {len(pos_dict1)}ê°œ (1-gram), {len(pos_dict2)}ê°œ (2-gram), {len(pos_dict3)}ê°œ (3-gram)\")\n",
    "print(f\"í™”ì¥í’ˆ íŠ¹í™” ë¶€ì •ì‚¬ì „: {len(neg_dict1)}ê°œ (1-gram), {len(neg_dict2)}ê°œ (2-gram), {len(neg_dict3)}ê°œ (3-gram)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ê²°í•©ëœ ê°ì„±ì‚¬ì „ ìƒì„± ===\n",
      "ê¸°ì¡´ KNU ê¸ì • ì‚¬ì „: 4868ê°œ\n",
      "í™”ì¥í’ˆ ê¸ì • ì‚¬ì „: 80ê°œ\n",
      "ê²°í•©ëœ ê¸ì • ì‚¬ì „: 4927ê°œ\n",
      "  â†’ ì¶”ê°€ëœ ë‹¨ì–´: 59ê°œ\n",
      "\n",
      "ê¸°ì¡´ KNU ë¶€ì • ì‚¬ì „: 9827ê°œ\n",
      "í™”ì¥í’ˆ ë¶€ì • ì‚¬ì „: 81ê°œ\n",
      "ê²°í•©ëœ ë¶€ì • ì‚¬ì „: 9901ê°œ\n",
      "  â†’ ì¶”ê°€ëœ ë‹¨ì–´: 74ê°œ\n",
      "\n",
      "âœ… ê²°í•©ëœ ì‚¬ì „ ì €ì¥ ì™„ë£Œ!\n",
      "  - combined_pos_words.txt: 4927ê°œ\n",
      "  - combined_neg_words.txt: 9901ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ì¡´ KNU ì‚¬ì „ + í™”ì¥í’ˆ íŠ¹í™” ì‚¬ì „ ê²°í•©\n",
    "def create_combined_sentiment_dictionary():\n",
    "    \"\"\"ê¸°ì¡´ KNU ì‚¬ì „ê³¼ í™”ì¥í’ˆ íŠ¹í™” ì‚¬ì „ì„ ê²°í•©\"\"\"\n",
    "    print(\"=== ê²°í•©ëœ ê°ì„±ì‚¬ì „ ìƒì„± ===\")\n",
    "    \n",
    "    # 1. ê¸°ì¡´ KNU ì‚¬ì „ ë¡œë“œ\n",
    "    with open('pos_pol_word.txt', 'r', encoding='utf-8') as f:\n",
    "        knu_pos_words = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    with open('neg_pol_word.txt', 'r', encoding='utf-8') as f:\n",
    "        knu_neg_words = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # 2. í™”ì¥í’ˆ íŠ¹í™” ì‚¬ì „ ë¡œë“œ\n",
    "    with open('cosmetic_pos_words.txt', 'r', encoding='utf-8') as f:\n",
    "        cosmetic_pos_words = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    with open('cosmetic_neg_words.txt', 'r', encoding='utf-8') as f:\n",
    "        cosmetic_neg_words = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # 3. ì‚¬ì „ ê²°í•© (ì¤‘ë³µ ì œê±°)\n",
    "    combined_pos_words = list(set(knu_pos_words + cosmetic_pos_words))\n",
    "    combined_neg_words = list(set(knu_neg_words + cosmetic_neg_words))\n",
    "    \n",
    "    print(f\"ê¸°ì¡´ KNU ê¸ì • ì‚¬ì „: {len(knu_pos_words)}ê°œ\")\n",
    "    print(f\"í™”ì¥í’ˆ ê¸ì • ì‚¬ì „: {len(cosmetic_pos_words)}ê°œ\")\n",
    "    print(f\"ê²°í•©ëœ ê¸ì • ì‚¬ì „: {len(combined_pos_words)}ê°œ\")\n",
    "    print(f\"  â†’ ì¶”ê°€ëœ ë‹¨ì–´: {len(combined_pos_words) - len(knu_pos_words)}ê°œ\")\n",
    "    \n",
    "    print(f\"\\nê¸°ì¡´ KNU ë¶€ì • ì‚¬ì „: {len(knu_neg_words)}ê°œ\")\n",
    "    print(f\"í™”ì¥í’ˆ ë¶€ì • ì‚¬ì „: {len(cosmetic_neg_words)}ê°œ\")\n",
    "    print(f\"ê²°í•©ëœ ë¶€ì • ì‚¬ì „: {len(combined_neg_words)}ê°œ\")\n",
    "    print(f\"  â†’ ì¶”ê°€ëœ ë‹¨ì–´: {len(combined_neg_words) - len(knu_neg_words)}ê°œ\")\n",
    "    \n",
    "    # 4. ê²°í•©ëœ ì‚¬ì „ ì €ì¥\n",
    "    with open('combined_pos_words.txt', 'w', encoding='utf-8') as f:\n",
    "        for word in combined_pos_words:\n",
    "            f.write(word + '\\n')\n",
    "    \n",
    "    with open('combined_neg_words.txt', 'w', encoding='utf-8') as f:\n",
    "        for word in combined_neg_words:\n",
    "            f.write(word + '\\n')\n",
    "    \n",
    "    print(f\"\\nâœ… ê²°í•©ëœ ì‚¬ì „ ì €ì¥ ì™„ë£Œ!\")\n",
    "    print(f\"  - combined_pos_words.txt: {len(combined_pos_words)}ê°œ\")\n",
    "    print(f\"  - combined_neg_words.txt: {len(combined_neg_words)}ê°œ\")\n",
    "    \n",
    "    return combined_pos_words, combined_neg_words\n",
    "\n",
    "# ê²°í•©ëœ ì‚¬ì „ ìƒì„±\n",
    "combined_pos, combined_neg = create_combined_sentiment_dictionary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ê²°í•©ëœ ì‚¬ì „ìœ¼ë¡œ total_test_tokens3.csv ìƒì„±\n",
    "# def create_total_test_tokens3_with_combined_dict():\n",
    "#     \"\"\"ê²°í•©ëœ ì‚¬ì „ì„ ì´ìš©í•œ total_test_tokens3.csv ìƒì„±\"\"\"\n",
    "#     print(\"=== ê²°í•©ëœ ì‚¬ì „ìœ¼ë¡œ total_test_tokens3.csv ìƒì„± ===\")\n",
    "    \n",
    "#     # 1. ë°ì´í„° ë¡œë“œ\n",
    "#     df = pd.read_csv('/Users/Shared/ìµœì¢…ì„ _êµìˆ˜ë‹˜/Face_skin_disease/ë°ì´í„° ì „ì²˜ë¦¬/Ntoken_review.csv', encoding='utf-8')\n",
    "#     print(f\"ì›ë³¸ ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
    "    \n",
    "#     # 2. ê²°í•©ëœ ì‚¬ì „ ë¡œë“œ\n",
    "#     with open('combined_pos_words.txt', 'r', encoding='utf-8') as f:\n",
    "#         combined_pos_words = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "#     with open('combined_neg_words.txt', 'r', encoding='utf-8') as f:\n",
    "#         combined_neg_words = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "#     print(f\"ê²°í•©ëœ ê¸ì • ì‚¬ì „: {len(combined_pos_words)}ê°œ\")\n",
    "#     print(f\"ê²°í•©ëœ ë¶€ì • ì‚¬ì „: {len(combined_neg_words)}ê°œ\")\n",
    "    \n",
    "#     # 3. Komoran ì´ˆê¸°í™”\n",
    "#     komoran = Komoran()\n",
    "#     tag_list = ['NNG', 'NNP', 'VV', 'VA', 'XR', 'MAG']\n",
    "    \n",
    "#     # 4. ê²°í•©ëœ ì‚¬ì „ì„ í˜•íƒœì†Œ ë¶„ì„í•˜ì—¬ ì •ì œ\n",
    "#     def process_dictionary(words, komoran, tag_list):\n",
    "#         \"\"\"ì‚¬ì „ì„ í˜•íƒœì†Œ ë¶„ì„í•˜ì—¬ ì •ì œ\"\"\"\n",
    "#         processed_words = []\n",
    "#         for word in words:\n",
    "#             try:\n",
    "#                 pos_result = komoran.pos(word)\n",
    "#                 filtered_tokens = [token[0] for token in pos_result if token[1] in tag_list]\n",
    "#                 if filtered_tokens:\n",
    "#                     processed_words.extend(filtered_tokens)\n",
    "#             except:\n",
    "#                 continue\n",
    "#         return list(set(processed_words))  # ì¤‘ë³µ ì œê±°\n",
    "    \n",
    "#     # ê²°í•©ëœ ì‚¬ì „ ì •ì œ\n",
    "#     combined_pos_processed = process_dictionary(combined_pos_words, komoran, tag_list)\n",
    "#     combined_neg_processed = process_dictionary(combined_neg_words, komoran, tag_list)\n",
    "    \n",
    "#     print(f\"ì •ì œëœ ê²°í•© ê¸ì • ì‚¬ì „: {len(combined_pos_processed)}ê°œ\")\n",
    "#     print(f\"ì •ì œëœ ê²°í•© ë¶€ì • ì‚¬ì „: {len(combined_neg_processed)}ê°œ\")\n",
    "    \n",
    "#     # 5. ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ í˜•íƒœì†Œ ë¶„ì„í•˜ê³  ê°ì„± ì ìˆ˜ ê³„ì‚°\n",
    "#     def calculate_sentiment_score(text, pos_words, neg_words, komoran, tag_list):\n",
    "#         \"\"\"ë¦¬ë·° í…ìŠ¤íŠ¸ì˜ ê°ì„± ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "#         if pd.isna(text):\n",
    "#             return 0.0\n",
    "        \n",
    "#         try:\n",
    "#             # í˜•íƒœì†Œ ë¶„ì„\n",
    "#             pos_result = komoran.pos(str(text))\n",
    "#             tokens = [token[0] for token in pos_result if token[1] in tag_list]\n",
    "            \n",
    "#             # ê°ì„± ë‹¨ì–´ ì¹´ìš´íŠ¸\n",
    "#             pos_count = sum(1 for token in tokens if token in pos_words)\n",
    "#             neg_count = sum(1 for token in tokens if token in neg_words)\n",
    "            \n",
    "#             # ê°ì„± ì ìˆ˜ ê³„ì‚° (ê¸ì • ë‹¨ì–´ - ë¶€ì • ë‹¨ì–´)\n",
    "#             total_words = len(tokens)\n",
    "#             if total_words == 0:\n",
    "#                 return 0.0\n",
    "            \n",
    "#             sentiment_score = (pos_count - neg_count) / total_words\n",
    "#             return sentiment_score\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "#             return 0.0\n",
    "    \n",
    "#     # 6. ê° ë¦¬ë·°ì— ëŒ€í•´ ê°ì„± ì ìˆ˜ ê³„ì‚°\n",
    "#     print(\"ë¦¬ë·° ê°ì„± ë¶„ì„ ì¤‘...\")\n",
    "#     sentiment_scores = []\n",
    "    \n",
    "#     for idx, row in df.iterrows():\n",
    "#         if idx % 100 == 0:\n",
    "#             print(f\"ì§„í–‰ë¥ : {idx}/{len(df)} ({idx/len(df)*100:.1f}%)\")\n",
    "        \n",
    "#         score = calculate_sentiment_score(\n",
    "#             row['review'], \n",
    "#             combined_pos_processed, \n",
    "#             combined_neg_processed, \n",
    "#             komoran, \n",
    "#             tag_list\n",
    "#         )\n",
    "#         sentiment_scores.append(score)\n",
    "    \n",
    "#     # 7. ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "#     result_df = pd.DataFrame({\n",
    "#         'sen': df['review'].tolist(),\n",
    "#         'pos_neg': sentiment_scores\n",
    "#     })\n",
    "    \n",
    "#     # 8. ê°ì„± ì ìˆ˜ ë¶„í¬ í™•ì¸\n",
    "#     print(\"\\n=== ê°ì„± ì ìˆ˜ ë¶„í¬ ===\")\n",
    "#     print(f\"í‰ê·  ê°ì„± ì ìˆ˜: {np.mean(sentiment_scores):.4f}\")\n",
    "#     print(f\"í‘œì¤€í¸ì°¨: {np.std(sentiment_scores):.4f}\")\n",
    "#     print(f\"ìµœì†Ÿê°’: {np.min(sentiment_scores):.4f}\")\n",
    "#     print(f\"ìµœëŒ“ê°’: {np.max(sentiment_scores):.4f}\")\n",
    "    \n",
    "#     # ê°ì„± ì ìˆ˜ë³„ ë¶„í¬\n",
    "#     positive_count = sum(1 for score in sentiment_scores if score > 0)\n",
    "#     negative_count = sum(1 for score in sentiment_scores if score < 0)\n",
    "#     neutral_count = sum(1 for score in sentiment_scores if score == 0)\n",
    "    \n",
    "#     print(f\"\\nê¸ì • (ì ìˆ˜ > 0): {positive_count}ê°œ ({positive_count/len(sentiment_scores)*100:.1f}%)\")\n",
    "#     print(f\"ë¶€ì • (ì ìˆ˜ < 0): {negative_count}ê°œ ({negative_count/len(sentiment_scores)*100:.1f}%)\")\n",
    "#     print(f\"ì¤‘ë¦½ (ì ìˆ˜ = 0): {neutral_count}ê°œ ({neutral_count/len(sentiment_scores)*100:.1f}%)\")\n",
    "    \n",
    "#     # 9. CSV íŒŒì¼ ì €ì¥\n",
    "#     output_file = 'total_test_tokens3.csv'\n",
    "#     result_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "#     print(f\"\\nâœ… {output_file} ì €ì¥ ì™„ë£Œ!\")\n",
    "#     print(f\"ì €ì¥ëœ ë°ì´í„° í¬ê¸°: {result_df.shape}\")\n",
    "    \n",
    "#     return result_df\n",
    "\n",
    "# # ì‹¤í–‰\n",
    "# total_test_tokens3 = create_total_test_tokens3_with_combined_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì„¸ ê°€ì§€ ì‚¬ì „ ì„±ëŠ¥ ë¹„êµ ===\n",
      "\n",
      "1. ê¸°ì¡´ KNU ì‚¬ì „ ì„±ëŠ¥:\n",
      "  ì‚¬ì „ í¬ê¸°: ê¸ì • 1716ê°œ, ë¶€ì • 2980ê°œ\n",
      "  ì •í™•ë„: 0.425\n",
      "\n",
      "2. í™”ì¥í’ˆ íŠ¹í™” ì‚¬ì „ ì„±ëŠ¥:\n",
      "  ì‚¬ì „ í¬ê¸°: ê¸ì • 35ê°œ, ë¶€ì • 31ê°œ\n",
      "  ì •í™•ë„: 0.751\n",
      "\n",
      "3. ê²°í•©ëœ ì‚¬ì „ ì„±ëŠ¥:\n",
      "  ì‚¬ì „ í¬ê¸°: ê¸ì • 1727ê°œ, ë¶€ì • 2994ê°œ\n",
      "  ì •í™•ë„: 0.518\n",
      "\n",
      "==================================================\n",
      "ğŸ“Š ì„±ëŠ¥ ë¹„êµ ìš”ì•½\n",
      "==================================================\n",
      "KNU         : 0.425\n",
      "í™”ì¥í’ˆ íŠ¹í™”      : 0.751\n",
      "ê²°í•©ëœ ì‚¬ì „      : 0.518\n",
      "\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥: í™”ì¥í’ˆ íŠ¹í™” (0.751)\n"
     ]
    }
   ],
   "source": [
    "# ì„¸ ê°€ì§€ ì‚¬ì „ ì„±ëŠ¥ ë¹„êµ ë¶„ì„\n",
    "def compare_sentiment_dictionaries():\n",
    "    \"\"\"ì„¸ ê°€ì§€ ì‚¬ì „ì˜ ì„±ëŠ¥ ë¹„êµ\"\"\"\n",
    "    print(\"=== ì„¸ ê°€ì§€ ì‚¬ì „ ì„±ëŠ¥ ë¹„êµ ===\")\n",
    "    \n",
    "    # 1. ë°ì´í„° ë¡œë“œ\n",
    "    df = pd.read_csv('/Users/Shared/ìµœì¢…ì„ _êµìˆ˜ë‹˜/Face_skin_disease/ë°ì´í„° ì „ì²˜ë¦¬/Ntoken_review.csv', encoding='utf-8')\n",
    "    \n",
    "    # 2. ì‹¤ì œ ê°ì„± ë ˆì´ë¸” ìƒì„± (í‰ì  ê¸°ë°˜)\n",
    "    df['actual_sentiment'] = df['rating'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "    \n",
    "    # 3. ê° ì‚¬ì „ë³„ ì„±ëŠ¥ í‰ê°€\n",
    "    results = {}\n",
    "    \n",
    "    # 3-1. ê¸°ì¡´ KNU ì‚¬ì „\n",
    "    print(\"\\n1. ê¸°ì¡´ KNU ì‚¬ì „ ì„±ëŠ¥:\")\n",
    "    with open('pos_pol_word.txt', 'r', encoding='utf-8') as f:\n",
    "        knu_pos = [line.strip() for line in f.readlines()]\n",
    "    with open('neg_pol_word.txt', 'r', encoding='utf-8') as f:\n",
    "        knu_neg = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    knu_accuracy = evaluate_dictionary_performance(df, knu_pos, knu_neg, \"KNU\")\n",
    "    results['KNU'] = knu_accuracy\n",
    "    \n",
    "    # 3-2. í™”ì¥í’ˆ íŠ¹í™” ì‚¬ì „\n",
    "    print(\"\\n2. í™”ì¥í’ˆ íŠ¹í™” ì‚¬ì „ ì„±ëŠ¥:\")\n",
    "    with open('cosmetic_pos_words.txt', 'r', encoding='utf-8') as f:\n",
    "        cosmetic_pos = [line.strip() for line in f.readlines()]\n",
    "    with open('cosmetic_neg_words.txt', 'r', encoding='utf-8') as f:\n",
    "        cosmetic_neg = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    cosmetic_accuracy = evaluate_dictionary_performance(df, cosmetic_pos, cosmetic_neg, \"í™”ì¥í’ˆ íŠ¹í™”\")\n",
    "    results['í™”ì¥í’ˆ íŠ¹í™”'] = cosmetic_accuracy\n",
    "    \n",
    "    # 3-3. ê²°í•©ëœ ì‚¬ì „\n",
    "    print(\"\\n3. ê²°í•©ëœ ì‚¬ì „ ì„±ëŠ¥:\")\n",
    "    with open('combined_pos_words.txt', 'r', encoding='utf-8') as f:\n",
    "        combined_pos = [line.strip() for line in f.readlines()]\n",
    "    with open('combined_neg_words.txt', 'r', encoding='utf-8') as f:\n",
    "        combined_neg = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    combined_accuracy = evaluate_dictionary_performance(df, combined_pos, combined_neg, \"ê²°í•©ëœ ì‚¬ì „\")\n",
    "    results['ê²°í•©ëœ ì‚¬ì „'] = combined_accuracy\n",
    "    \n",
    "    # 4. ê²°ê³¼ ìš”ì•½\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ“Š ì„±ëŠ¥ ë¹„êµ ìš”ì•½\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for name, accuracy in results.items():\n",
    "        print(f\"{name:12}: {accuracy:.3f}\")\n",
    "    \n",
    "    # ìµœê³  ì„±ëŠ¥ ì‚¬ì „ ì°¾ê¸°\n",
    "    best_dict = max(results, key=results.get)\n",
    "    print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥: {best_dict} ({results[best_dict]:.3f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_dictionary_performance(df, pos_words, neg_words, dict_name):\n",
    "    \"\"\"ì‚¬ì „ ì„±ëŠ¥ í‰ê°€\"\"\"\n",
    "    komoran = Komoran()\n",
    "    tag_list = ['NNG', 'NNP', 'VV', 'VA', 'XR', 'MAG']\n",
    "    \n",
    "    # ì‚¬ì „ ì •ì œ\n",
    "    def process_dictionary(words, komoran, tag_list):\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                pos_result = komoran.pos(word)\n",
    "                filtered_tokens = [token[0] for token in pos_result if token[1] in tag_list]\n",
    "                if filtered_tokens:\n",
    "                    processed_words.extend(filtered_tokens)\n",
    "            except:\n",
    "                continue\n",
    "        return list(set(processed_words))\n",
    "    \n",
    "    pos_processed = process_dictionary(pos_words, komoran, tag_list)\n",
    "    neg_processed = process_dictionary(neg_words, komoran, tag_list)\n",
    "    \n",
    "    # ê°ì„± ë¶„ì„\n",
    "    def simple_sentiment_analysis(text, pos_words, neg_words):\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            pos_result = komoran.pos(str(text))\n",
    "            tokens = [token[0] for token in pos_result if token[1] in tag_list]\n",
    "            \n",
    "            pos_count = sum(1 for token in tokens if token in pos_words)\n",
    "            neg_count = sum(1 for token in tokens if token in neg_words)\n",
    "            \n",
    "            if pos_count > neg_count:\n",
    "                return 1\n",
    "            elif neg_count > pos_count:\n",
    "                return 0\n",
    "            else:\n",
    "                return 0.5\n",
    "        except:\n",
    "            return 0.5\n",
    "    \n",
    "    # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "    df['predicted'] = df['review'].apply(\n",
    "        lambda x: simple_sentiment_analysis(x, pos_processed, neg_processed)\n",
    "    )\n",
    "    \n",
    "    # ì •í™•ë„ ê³„ì‚°\n",
    "    accuracy = (df['predicted'] == df['actual_sentiment']).mean()\n",
    "    \n",
    "    print(f\"  ì‚¬ì „ í¬ê¸°: ê¸ì • {len(pos_processed)}ê°œ, ë¶€ì • {len(neg_processed)}ê°œ\")\n",
    "    print(f\"  ì •í™•ë„: {accuracy:.3f}\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰\n",
    "comparison_results = compare_sentiment_dictionaries()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
